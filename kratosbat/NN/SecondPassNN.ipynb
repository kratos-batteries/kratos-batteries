{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 8075.4755859375\n",
      "199 7764.99560546875\n",
      "299 6988.41943359375\n",
      "399 6006.9443359375\n",
      "499 5055.3681640625\n",
      "599 4557.8203125\n",
      "699 4773.828125\n",
      "799 3752.041259765625\n",
      "899 4044.66015625\n",
      "999 3387.39501953125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#categorizing the input and output data\n",
    "bat_data = pd.read_csv('NEWTrainingData_StandardScaler.csv')\n",
    "train_bat = bat_data\n",
    "\n",
    "D_in, H, H2, D_out, N = 165, 100, 75, 2, 4000\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "#define testing sets\n",
    "x_train = train_bat.drop(columns=['Unnamed: 0', 'Gravimetric Capacity (units)', 'Volumetric Capacity', 'Max Delta Volume'])\n",
    "y_train = train_bat[['Gravimetric Capacity (units)', 'Volumetric Capacity']]\n",
    "\n",
    "#test-train split\n",
    "#shuffle\n",
    "x_train = x_train.sample(frac = 1)\n",
    "y_train = y_train.sample(frac = 1)\n",
    "\n",
    "#optimize for user data\n",
    "x_test = x_train[4000:]\n",
    "y_test = y_train[4000:]\n",
    "\n",
    "x_train = x_train[:4000]\n",
    "y_train = y_train[:4000]\n",
    "\n",
    "#Defining training and testing data\n",
    "x_train_np = np.array(x_train)\n",
    "x_train_torch = torch.tensor(x_train_np, device = device, dtype = dtype)\n",
    "\n",
    "y_train_np = np.array(y_train)\n",
    "y_train_torch = torch.tensor(y_train_np, device = device, dtype = dtype)\n",
    "\n",
    "x_test_np = np.array(x_test)\n",
    "x_test_torch = torch.tensor(x_test_np, device = device, dtype = dtype)\n",
    "\n",
    "y_test_np = np.array(y_test)\n",
    "y_test_torch = torch.tensor(y_test_np, device = device, dtype = dtype)\n",
    "\n",
    "#Defining weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "#w3 = torch.randn(H2, D_out, device=device, dtype=dtype)\n",
    "\n",
    "#define model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, D_out),\n",
    "    #nn.Softmax(dim=1) #normalizing the data,\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-5\n",
    "for t in range(1000):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x_train_torch)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_train_torch)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward/training pass.\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(532.8734, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model(x_test_torch)\n",
    "loss_fn(y_pred_test, y_test_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3915.1335, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "RSS = ((model(x_train_torch)-y_train_torch)**2).sum()\n",
    "print(RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 38.27134323120117\n",
      "199 37.96364974975586\n",
      "299 37.73960876464844\n",
      "399 37.537025451660156\n",
      "499 37.34566116333008\n",
      "599 37.15192794799805\n",
      "699 36.94499206542969\n",
      "799 36.73301315307617\n",
      "899 36.527015686035156\n",
      "999 37.5709342956543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#categorizing the input and output data\n",
    "bat_data = pd.read_csv('NEWTrainingData_MinMaxScaler.csv')\n",
    "train_bat = bat_data\n",
    "\n",
    "D_in, H, H2, D_out, N = 115, 100, 75, 2, 4000\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "#define testing sets\n",
    "x_train = train_bat.drop(columns=['Unnamed: 0', 'Gravimetric Capacity (units)', 'Volumetric Capacity', 'Max Delta Volume'])\n",
    "y_train = train_bat[['Gravimetric Capacity (units)', 'Volumetric Capacity']]\n",
    "\n",
    "#shuffle data\n",
    "x_train = x_train.sample(frac=1)\n",
    "y_train = y_train.sample(frac=1)\n",
    "\n",
    "#test-train split\n",
    "#optimize for user data\n",
    "x_test = x_train[4000:]\n",
    "y_test = y_train[4000:]\n",
    "\n",
    "x_train = x_train[:4000]\n",
    "y_train = y_train[:4000]\n",
    "\n",
    "#Defining training and testing data\n",
    "x_train_np = np.array(x_train)\n",
    "x_train_torch = torch.tensor(x_train_np, device = device, dtype = dtype)\n",
    "\n",
    "y_train_np = np.array(y_train)\n",
    "y_train_torch = torch.tensor(y_train_np, device = device, dtype = dtype)\n",
    "\n",
    "x_test_np = np.array(x_test)\n",
    "x_test_torch = torch.tensor(x_test_np, device = device, dtype = dtype)\n",
    "\n",
    "y_test_np = np.array(y_test)\n",
    "y_test_torch = torch.tensor(y_test_np, device = device, dtype = dtype)\n",
    "\n",
    "#Defining weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "#w3 = torch.randn(H2, D_out, device=device, dtype=dtype)\n",
    "\n",
    "#define model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, D_out),\n",
    "    #nn.Softmax(dim=1) #normalizing the data,\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(1000):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x_train_torch)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_train_torch)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward/training pass.\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0370, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model(x_test_torch)\n",
    "loss_fn(y_test_torch, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.6390, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "RSS = ((model(x_train_torch)-y_train_torch)**2).sum()\n",
    "print(RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Volume Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>Gravimetric Capacity (units)</th>\n",
       "      <th>Volumetric Capacity</th>\n",
       "      <th>Max Delta Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.486246</td>\n",
       "      <td>0.060893</td>\n",
       "      <td>-0.214474</td>\n",
       "      <td>-0.112607</td>\n",
       "      <td>0.675571</td>\n",
       "      <td>0.298940</td>\n",
       "      <td>1.110048</td>\n",
       "      <td>-0.409336</td>\n",
       "      <td>-0.608829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>-0.004783</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.032282</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.694759</td>\n",
       "      <td>-1.063526</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>-0.221694</td>\n",
       "      <td>-0.357235</td>\n",
       "      <td>0.335010</td>\n",
       "      <td>0.456398</td>\n",
       "      <td>-0.564281</td>\n",
       "      <td>-0.188967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>-0.006621</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.679393</td>\n",
       "      <td>-0.770838</td>\n",
       "      <td>-0.507284</td>\n",
       "      <td>0.441374</td>\n",
       "      <td>0.449890</td>\n",
       "      <td>0.084205</td>\n",
       "      <td>0.534985</td>\n",
       "      <td>-0.494704</td>\n",
       "      <td>0.691186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.008098</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.008740</td>\n",
       "      <td>-0.041023</td>\n",
       "      <td>-0.032473</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.069254</td>\n",
       "      <td>0.082636</td>\n",
       "      <td>0.000228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.222118</td>\n",
       "      <td>-0.352762</td>\n",
       "      <td>-1.097785</td>\n",
       "      <td>0.364854</td>\n",
       "      <td>-0.194111</td>\n",
       "      <td>0.090228</td>\n",
       "      <td>0.444386</td>\n",
       "      <td>-0.425387</td>\n",
       "      <td>-0.030196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>-0.004796</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.026762</td>\n",
       "      <td>0.040243</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>-1.018132</td>\n",
       "      <td>0.039497</td>\n",
       "      <td>-0.122775</td>\n",
       "      <td>-0.261139</td>\n",
       "      <td>0.298638</td>\n",
       "      <td>0.410037</td>\n",
       "      <td>-0.529058</td>\n",
       "      <td>-0.159984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012249</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>-0.004106</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.026762</td>\n",
       "      <td>0.037129</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>4396</td>\n",
       "      <td>-0.108310</td>\n",
       "      <td>0.940409</td>\n",
       "      <td>0.959655</td>\n",
       "      <td>1.113824</td>\n",
       "      <td>-0.307627</td>\n",
       "      <td>-0.127549</td>\n",
       "      <td>-0.372501</td>\n",
       "      <td>0.182058</td>\n",
       "      <td>0.143321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013122</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>-0.032890</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>0.034519</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>-0.013542</td>\n",
       "      <td>0.034073</td>\n",
       "      <td>0.057571</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>4397</td>\n",
       "      <td>0.731422</td>\n",
       "      <td>0.527013</td>\n",
       "      <td>0.328726</td>\n",
       "      <td>0.594572</td>\n",
       "      <td>-0.452792</td>\n",
       "      <td>-0.342455</td>\n",
       "      <td>0.388687</td>\n",
       "      <td>0.537657</td>\n",
       "      <td>0.289902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010045</td>\n",
       "      <td>-0.012350</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.021788</td>\n",
       "      <td>0.047893</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>4398</td>\n",
       "      <td>0.522887</td>\n",
       "      <td>1.133211</td>\n",
       "      <td>-0.909830</td>\n",
       "      <td>-0.269775</td>\n",
       "      <td>-0.692468</td>\n",
       "      <td>0.249483</td>\n",
       "      <td>0.157622</td>\n",
       "      <td>0.250940</td>\n",
       "      <td>-0.109424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.001908</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>0.051223</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>4399</td>\n",
       "      <td>-0.297020</td>\n",
       "      <td>1.228747</td>\n",
       "      <td>-0.548483</td>\n",
       "      <td>-0.237261</td>\n",
       "      <td>-0.713716</td>\n",
       "      <td>-0.278450</td>\n",
       "      <td>0.154214</td>\n",
       "      <td>-0.274529</td>\n",
       "      <td>-0.121010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011201</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>-0.011007</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>-0.001337</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>0.041988</td>\n",
       "      <td>0.093676</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>4400</td>\n",
       "      <td>1.160439</td>\n",
       "      <td>0.617236</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>-0.033884</td>\n",
       "      <td>-0.079313</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>0.201203</td>\n",
       "      <td>-0.085531</td>\n",
       "      <td>0.153455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019173</td>\n",
       "      <td>-0.009836</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>-0.019444</td>\n",
       "      <td>-0.002352</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>0.144553</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0              0  0.486246  0.060893 -0.214474 -0.112607  0.675571  0.298940   \n",
       "1              1  0.694759 -1.063526  0.001798 -0.221694 -0.357235  0.335010   \n",
       "2              2  0.679393 -0.770838 -0.507284  0.441374  0.449890  0.084205   \n",
       "3              3  0.222118 -0.352762 -1.097785  0.364854 -0.194111  0.090228   \n",
       "4              4  0.496687 -1.018132  0.039497 -0.122775 -0.261139  0.298638   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "4396        4396 -0.108310  0.940409  0.959655  1.113824 -0.307627 -0.127549   \n",
       "4397        4397  0.731422  0.527013  0.328726  0.594572 -0.452792 -0.342455   \n",
       "4398        4398  0.522887  1.133211 -0.909830 -0.269775 -0.692468  0.249483   \n",
       "4399        4399 -0.297020  1.228747 -0.548483 -0.237261 -0.713716 -0.278450   \n",
       "4400        4400  1.160439  0.617236 -0.001714 -0.033884 -0.079313  0.030198   \n",
       "\n",
       "             6         7         8  ...       108       109       110  \\\n",
       "0     1.110048 -0.409336 -0.608829  ...  0.013285  0.008151 -0.004783   \n",
       "1     0.456398 -0.564281 -0.188967  ...  0.006995  0.012646  0.007878   \n",
       "2     0.534985 -0.494704  0.691186  ...  0.013040  0.008098 -0.001060   \n",
       "3     0.444386 -0.425387 -0.030196  ...  0.011135  0.010524 -0.000659   \n",
       "4     0.410037 -0.529058 -0.159984  ...  0.012249  0.011378 -0.004106   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4396 -0.372501  0.182058  0.143321  ...  0.013122  0.009733 -0.032890   \n",
       "4397  0.388687  0.537657  0.289902  ... -0.010045 -0.012350  0.003835   \n",
       "4398  0.157622  0.250940 -0.109424  ... -0.005085  0.006921  0.001318   \n",
       "4399  0.154214 -0.274529 -0.121010  ...  0.011201  0.015631  0.007458   \n",
       "4400  0.201203 -0.085531  0.153455  ... -0.019173 -0.009836  0.001286   \n",
       "\n",
       "           111       112       113       114  Gravimetric Capacity (units)  \\\n",
       "0     0.001267 -0.005504  0.005775  0.002805                      0.017651   \n",
       "1    -0.006621 -0.010773  0.004906  0.009384                      0.013516   \n",
       "2    -0.008740 -0.041023 -0.032473 -0.005981                      0.069254   \n",
       "3    -0.004796  0.001783  0.001435  0.005299                      0.026762   \n",
       "4    -0.003080  0.001072 -0.001411  0.004857                      0.026762   \n",
       "...        ...       ...       ...       ...                           ...   \n",
       "4396 -0.021991  0.034519  0.002598 -0.013542                      0.034073   \n",
       "4397  0.012596  0.004243  0.011012  0.009617                      0.021788   \n",
       "4398  0.002289 -0.006700 -0.001908 -0.002679                      0.051223   \n",
       "4399 -0.011007 -0.004750 -0.001337 -0.000638                      0.041988   \n",
       "4400  0.003553 -0.019444 -0.002352 -0.002160                      0.072047   \n",
       "\n",
       "      Volumetric Capacity  Max Delta Volume  \n",
       "0                0.032282          0.000107  \n",
       "1                0.017134          0.000066  \n",
       "2                0.082636          0.000228  \n",
       "3                0.040243          0.000079  \n",
       "4                0.037129          0.000095  \n",
       "...                   ...               ...  \n",
       "4396             0.057571          0.000260  \n",
       "4397             0.047893          0.000024  \n",
       "4398             0.117200          0.000217  \n",
       "4399             0.093676          0.000072  \n",
       "4400             0.144553          0.000012  \n",
       "\n",
       "[4401 rows x 119 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 7296.1962890625\n",
      "199 7216.1279296875\n",
      "299 7162.02880859375\n",
      "399 7143.966796875\n",
      "499 7132.01953125\n",
      "599 7123.76220703125\n",
      "699 7117.7119140625\n",
      "799 7112.9736328125\n",
      "899 7109.189453125\n",
      "999 7106.0634765625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#categorizing the input and output data\n",
    "bat_data = pd.read_csv('NEWTrainingData_MinMaxScaler.csv')\n",
    "train_bat = bat_data\n",
    "\n",
    "D_in, H, H2, D_out, N = 2, 100, 75, 1, 4000\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "#define testing sets\n",
    "x_train = train_bat[['Gravimetric Capacity (units)', 'Volumetric Capacity']]\n",
    "y_train = train_bat['Max Delta Volume']\n",
    "\n",
    "#shuffle data\n",
    "x_train = x_train.sample(frac=1)\n",
    "y_train = y_train.sample(frac=1)\n",
    "\n",
    "#test-train split\n",
    "#optimize for user data\n",
    "x_test = x_train[4000:]\n",
    "y_test = y_train[4000:]\n",
    "\n",
    "x_train = x_train[:4000]\n",
    "y_train = y_train[:4000]\n",
    "\n",
    "#Defining training and testing data\n",
    "x_train_np = np.array(x_train)\n",
    "x_train_torch = torch.tensor(x_train_np, device = device, dtype = dtype)\n",
    "\n",
    "y_train_np = np.array(y_train)\n",
    "y_train_torch = torch.tensor(y_train_np, device = device, dtype = dtype)\n",
    "\n",
    "x_test_np = np.array(x_test)\n",
    "x_test_torch = torch.tensor(x_test_np, device = device, dtype = dtype)\n",
    "\n",
    "y_test_np = np.array(y_test)\n",
    "y_test_torch = torch.tensor(y_test_np, device = device, dtype = dtype)\n",
    "\n",
    "#Defining weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "#w3 = torch.randn(H2, D_out, device=device, dtype=dtype)\n",
    "\n",
    "#define model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, D_out),\n",
    "    #nn.Softmax(dim=1) #normalizing the data,\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-8)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-8\n",
    "for t in range(1000):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x_train_torch)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_train_torch)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward/training pass.\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewgonsalves/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([401, 1])) that is different to the input size (torch.Size([401])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.6999, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model(x_test_torch)\n",
    "loss_fn(y_test_torch, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7106.0352, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "RSS = ((model(x_train_torch)-y_train_torch)**2).sum()\n",
    "print(RSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
